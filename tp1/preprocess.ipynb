{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_sm\")                          #en_core_web_sm est un modèle linguistique pré-entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" doc = nlp('This is a sentence.')\\nfor token in doc:\\n    print ( token.text, token.pos_, token.dep_)    \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" doc = nlp('This is a sentence.')\n",
    "for token in doc:\n",
    "    print ( token.text, token.pos_, token.dep_)    \"\"\"                                      #Pos part of speech la categorie gramaticale      dep la relation syntaxique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‘d', 'sixty', 'that', 'not', 'latter', 'very', 'elsewhere', 'various', 'he', 'except', 'are', 'these', 'afterwards', 'toward', 'they', 'seems', 'it', 'keep', 'himself', 'just', 'towards', 'from', 'other', 'mine', 'mostly', 'still', 'our', 'call', 'six', 'themselves', 'than', 'already', 'seem', 'whom', '’m', 'such', 'to', 'rather', 'me', 'first', 'as', 'nevertheless', 'bottom', 'most', '’re', 'beyond', 'give', 'hereby', 'should', 'might', 'unless', 'on', 'show', 'hereupon', 'besides', 'when', 'four', 'neither', 'about', '‘ll', 'my', 'though', 'eleven', 'yours', 'since', '’ll', 'behind', 'herself', 'herein', 'otherwise', 'less', 'indeed', 'wherever', 'we', 'every', 'third', 'thru', 'moreover', 'was', \"'ve\", 'between', 'their', 'quite', '’s', 'full', 'under', 'could', 'something', 'together', 'this', 'whoever', 'another', 'next', 'former', 'therein', 'anyway', 'can', 'sometime', 'back', 'below', 'latterly', 'alone', 'and', 'somehow', 'into', \"'s\", 'almost', 'upon', 'move', 'above', 'anything', 'twelve', 'anyhow', 'what', 'if', 'forty', 'has', 'see', 'for', 'will', 'one', 'thereafter', 'only', 'without', 'do', 'beforehand', 'many', 'whereby', 'wherein', 'hence', 'beside', 'may', 'along', \"'d\", 'several', 'even', 'everyone', 'everywhere', 'own', 'two', 'whereas', 'thus', 'there', 'down', 'i', 'used', 'get', 'although', 'side', 'whereupon', 'itself', 'however', 'any', 're', 'hers', 'after', 'also', 'anywhere', 'became', 'within', 'whether', 'whence', 'empty', 'eight', 'am', 'amongst', 'due', 'fifteen', 'no', 'too', 'nobody', 'either', '‘re', 'further', 'thereupon', 'never', 'go', 'out', 'meanwhile', 'whose', 'over', 'made', 'front', 'hereafter', 'an', 'his', 'others', 'yet', 'must', 'noone', 'once', 'him', 'during', 'anyone', 'becoming', 'where', 'around', \"'ll\", '‘s', '‘ve', 'those', 'thereby', 'whereafter', 'does', 'really', 'in', 'why', \"'re\", 'none', 'five', 'cannot', 'among', 'throughout', 'enough', 'she', 'or', 'somewhere', 'n‘t', 'how', 'until', 'some', 'whole', 'against', 'thence', 'onto', 'been', 'part', 'n’t', 'amount', 'myself', 'did', 'before', 'at', 'top', 'but', 'take', 'you', '’ve', 'say', 'ourselves', 'being', 'then', 'doing', 'while', 'always', 'everything', '’d', 'make', 'the', 'name', 'nothing', 'put', 'yourselves', 'its', 'perhaps', 'much', 'therefore', 'whenever', 'ever', 'ten', 'few', 'becomes', 'serious', 'yourself', 'formerly', 'please', 'which', 'were', 'each', \"n't\", 'your', 'nor', 'up', 'last', 'become', 'who', 'same', 'by', \"'m\", 'hundred', 'seemed', 'nowhere', 'because', 'seeming', 'here', 'all', 'often', 'now', 'someone', 'have', 'again', 'us', 'regarding', 'with', '‘m', 'be', 'sometimes', 'of', 'them', 'well', 'least', 'twenty', 'per', 'is', 'a', 'so', 'through', 'nine', 'via', 'whatever', 'fifty', 'whither', 'ca', 'would', 'using', 'both', 'more', 'three', 'ours', 'done', 'else', 'her', 'off', 'had', 'across', 'namely'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' for word in my_stop_words:\\n    STOP_WORDS.add(word)\\n\\nprint (STOP_WORDS) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from spacy.lang.af import stop_words\n",
    "\n",
    "\n",
    "#my_stop_words = ['say', 'said', 'field', 'be','fine','mdr']\n",
    "\n",
    "\"\"\" for word in my_stop_words:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True \"\"\"\n",
    "print (STOP_WORDS)\n",
    "\n",
    "\"\"\" for word in my_stop_words:\n",
    "    STOP_WORDS.add(word)\n",
    "\n",
    "print (STOP_WORDS) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenisation du texte\n",
    "    doc = nlp(text.lower())  # Minuscule\n",
    "\n",
    "    # Extraction des mots sans ponctuations, stopwords et sans chiffres\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_digit]\n",
    "    \n",
    "    # Compter la fréquence des mots\n",
    "    word_freq = Counter([token.text for token in tokens])\n",
    "    \n",
    "    # Déterminer les seuils en fonction de la fréquence maximale\n",
    "    max_frequency = max(word_freq.values())\n",
    "    \n",
    "    # Définir les seuils pour les mots fréquents et rares\n",
    "    frequent_threshold = max_frequency * 0.1  # 10% de la fréquence maximale\n",
    "    rare_threshold = max_frequency * 0.01  # 1% de la fréquence maximale\n",
    "\n",
    "    # Identification des mots fréquents et rares\n",
    "    frequent_words = {word for word, freq in word_freq.items() if freq > frequent_threshold}\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq <= rare_threshold}\n",
    "\n",
    "    # Appliquer la lemmatisation et filtrer les mots fréquents et rares\n",
    "    lemmatized_tokens = [token.lemma_ for token in tokens if token.text not in frequent_words and token.text not in rare_words]\n",
    "\n",
    "    # Retourner les tokens nettoyés\n",
    "    return \" \".join(lemmatized_tokens) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte avant prétraitement :\n",
      " \n",
      "In the heart of the city lies a park where people gather to relax and unwind. The trees sway gently in the breeze,\n",
      "and the sound of birds chirping fills the air. Every morning, joggers take to the paths, their footsteps blending \n",
      "with the natural symphony of the park. Children play on the swings while their parents watch from nearby benches. \n",
      "On weekends, local vendors set up stalls selling fresh fruits, homemade crafts, and colorful flowers. The park \n",
      "has become a sanctuary for city dwellers, offering them a brief escape from the noise and bustle of urban life.\n",
      "Yet, as peaceful as it seems, the park holds a history not many know about. Decades ago, this very spot was a \n",
      "barren lot, slated for commercial development. But the community came together, rallied against the plan, and \n",
      "fought to preserve this space as a public park. Their efforts succeeded, and now the park stands as a symbol of \n",
      "what people can achieve when they work together for a common cause. It's a reminder that even in the midst of progress, \n",
      "there's room for nature and serenity.\n",
      "UK 'risks breaking golden rule'\n",
      "The UK government will have to raise taxes or rein in spending if it wants to avoid breaking its \"golden rule\", a report suggests.\n",
      "The rule states that the government can borrow cash only to invest, and not to finance its spending projects. The National Institute of Economic and Social Research (NIESR) claims that taxes need to rise by about £10bn if state finances are to be put in order. The Treasury said its plans were on track and funded until 2008. According to NIESR, if the government's current economic cycle runs until March 2006 then it is \"unlikely\" the golden rule will be met. Should the cycle end a year earlier, then the chances improve to \"50/50\". Either way, fiscal tightening is needed, NIESR said.\n",
      "The report is the latest to call into question the viability of government spending projections. Earlier this month, accountancy firm Ernst & Young said that Chancellor of the Exchequer Gordon Brown's forecasts for tax revenues were too optimistic.\n",
      "It claimed revenues were likely to be £6bn below estimates by the end of the tax year despite the economy growing in line with forecasts. A Treasury spokesperson dismissed the latest claims, saying it was \"on track to meeting spending rules and the golden rule in the current cycle and beyond\". \"Spending plans have been set out until 2008 and they are fully affordable.\" Other than its warning on possible tax hikes, the NIESR report was optimistic about the state of the UK and global economy.\n",
      "It said the recent record-busting surge in oil prices would have a limited effect on worldwide expansion, saying that if anything the \"world economy will continue to grow strongly\". Global gross domestic product (GDP) is tipped to be 4.1% this year, dipping to 4% in 2005, before picking up again to 4.2% in 2006. The US will continue to drive expansion until 2006, albeit at a slightly slower rate, as will be the case in Japan. Hinting at better times for UK exporters, NIESR said the euro zone \"is expected to pick up speed\".\n",
      "\n",
      "\n",
      "Texte après prétraitement :\n",
      " heart lie gather relax unwind tree sway gently breeze sound bird chirp fill air morning jogger path footstep blend natural symphony child play swing parent watch nearby bench weekend local vendor stall sell fresh fruit homemade craft colorful flower sanctuary dweller offer brief escape noise bustle urban life peaceful hold history know decade ago spot barren lot slate commercial development community come rally plan fight preserve space public effort succeed stand symbol achieve work common cause reminder midst progress room nature serenity risk raise rein want avoid suggest state borrow cash invest finance project national institute social research need rise 10bn finance order fund accord run march unlikely meet chance improve 50/50 way fiscal tightening need question viability projection month accountancy firm ernst young chancellor exchequer gordon brown claim likely 6bn estimate despite grow line spokesperson dismiss meet rule fully affordable warning possible hike recent record bust surge oil price limited effect worldwide world grow strongly gross domestic product gdp tip 4.1 dip pick 4.2 drive albeit slightly slow rate case japan hint well time exporter euro zone expect pick speed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemple de texte\n",
    "text = \"\"\"\n",
    "In the heart of the city lies a park where people gather to relax and unwind. The trees sway gently in the breeze,\n",
    "and the sound of birds chirping fills the air. Every morning, joggers take to the paths, their footsteps blending \n",
    "with the natural symphony of the park. Children play on the swings while their parents watch from nearby benches. \n",
    "On weekends, local vendors set up stalls selling fresh fruits, homemade crafts, and colorful flowers. The park \n",
    "has become a sanctuary for city dwellers, offering them a brief escape from the noise and bustle of urban life.\n",
    "Yet, as peaceful as it seems, the park holds a history not many know about. Decades ago, this very spot was a \n",
    "barren lot, slated for commercial development. But the community came together, rallied against the plan, and \n",
    "fought to preserve this space as a public park. Their efforts succeeded, and now the park stands as a symbol of \n",
    "what people can achieve when they work together for a common cause. It's a reminder that even in the midst of progress, \n",
    "there's room for nature and serenity.\n",
    "UK 'risks breaking golden rule'\n",
    "The UK government will have to raise taxes or rein in spending if it wants to avoid breaking its \"golden rule\", a report suggests.\n",
    "The rule states that the government can borrow cash only to invest, and not to finance its spending projects. The National Institute of Economic and Social Research (NIESR) claims that taxes need to rise by about £10bn if state finances are to be put in order. The Treasury said its plans were on track and funded until 2008. According to NIESR, if the government's current economic cycle runs until March 2006 then it is \"unlikely\" the golden rule will be met. Should the cycle end a year earlier, then the chances improve to \"50/50\". Either way, fiscal tightening is needed, NIESR said.\n",
    "The report is the latest to call into question the viability of government spending projections. Earlier this month, accountancy firm Ernst & Young said that Chancellor of the Exchequer Gordon Brown's forecasts for tax revenues were too optimistic.\n",
    "It claimed revenues were likely to be £6bn below estimates by the end of the tax year despite the economy growing in line with forecasts. A Treasury spokesperson dismissed the latest claims, saying it was \"on track to meeting spending rules and the golden rule in the current cycle and beyond\". \"Spending plans have been set out until 2008 and they are fully affordable.\" Other than its warning on possible tax hikes, the NIESR report was optimistic about the state of the UK and global economy.\n",
    "It said the recent record-busting surge in oil prices would have a limited effect on worldwide expansion, saying that if anything the \"world economy will continue to grow strongly\". Global gross domestic product (GDP) is tipped to be 4.1% this year, dipping to 4% in 2005, before picking up again to 4.2% in 2006. The US will continue to drive expansion until 2006, albeit at a slightly slower rate, as will be the case in Japan. Hinting at better times for UK exporters, NIESR said the euro zone \"is expected to pick up speed\".\n",
    "\"\"\"\n",
    "doc = nlp (text)\n",
    "# Prétraitement du texte\n",
    "cleaned_text = preprocess_text(text)\n",
    "\n",
    "print(\"Texte avant prétraitement :\\n\", text)\n",
    "print(\"\\nTexte après prétraitement :\\n\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "45.6 7.6000000000000005\n",
      "Texte après prétraitement :\n",
      " fire gare dead dead ser waymar royce gare man dead dead say dead dead royce gare say dead dead say royce dead gare ser waymar royce gare gare black gare man man cold like cold gare like ser waymar royce black gare black black black black black ser waymar black gare man gare say gare say dead like like black black say fire man man like royce royce man royce gare ser waymar cold gare say cold like cold like like gare ser waymar cold gare s waymar ser waymar gare gare gare say cold man say cold royce cold fire dead ser waymar royce black gare man black royce say say ser waymar royce ser waymar royce cold like gare gare gare gare royce say gare gare fire man fire fire gare say ser waymar fire gare man gare gare fire royce say\n",
      "le nombre de mots contenus dans le texte apres preprocessing : 144\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Charger le modèle Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Fonction de preprocessing\n",
    "def preprocess_text(text, frequent_threshold_percent=30, rare_threshold_percent=5):\n",
    "    # Tokenisation du texte\n",
    "    doc = nlp(text.lower())  # Minuscule\n",
    "    \n",
    "    # Extraction des tokens sans stopwords, ponctuations, et sans chiffres\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_digit]\n",
    "    \n",
    "    # Compter la fréquence des mots\n",
    "    word_freq = Counter([token.text for token in tokens])\n",
    "    \n",
    "    # Déterminer la fréquence maximale\n",
    "    max_frequency = max(word_freq.values())\n",
    "    print (max_frequency)\n",
    "    \n",
    "    # Seuils basés sur des pourcentages\n",
    "    frequent_threshold = (frequent_threshold_percent / 100) * max_frequency\n",
    "    rare_threshold = (rare_threshold_percent / 100) * max_frequency\n",
    "\n",
    "    print (frequent_threshold, rare_threshold)\n",
    "\n",
    "    # Identification des mots fréquents et rares\n",
    "    frequent_words = {word for word, freq in word_freq.items() if freq > frequent_threshold}\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq <= rare_threshold}\n",
    "\n",
    "    # Appliquer la lemmatisation et filtrer les mots fréquents et rares\n",
    "    lemmatized_tokens = [token.lemma_ for token in tokens if token.text not in frequent_words and token.text not in rare_words]\n",
    "    \n",
    "    # Retourner le texte nettoyé\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Lire le fichier txt\n",
    "with open(\"game_of_thrones.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "# Appliquer le prétraitement au texte\n",
    "cleaned_text = preprocess_text(text)\n",
    "\n",
    "word_count = len(cleaned_text.split())\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Texte après prétraitement :\\n\", cleaned_text)  # Affichage des 1000 premiers caractères pour vérifier\n",
    "print (f\"le nombre de mots contenus dans le texte apres preprocessing : {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_frequency: 152\n",
      "la frequence maximale:45.6 , rare_threshold: 7.6000000000000005\n",
      "Texte après prétraitement :\n",
      " fire gare dead dead ser waymar royce gare man see come dead dead say dead dead royce see gare say say dead dead man say royce dead gare ser waymar royce gare gare black gare watch man man come come come cold like watch cold gare like ser waymar royce black gare black black black black black ser waymar watch black gare man gare say gare say dead like snow snow see like see watch black black say man snow fire watch man man like royce see see royce man royce man gare ser waymar cold gare say see man snow come cold fire like cold like like gare ser waymar cold gare waymar watch ser waymar gare gare come gare say cold watch watch man say see cold royce snow cold man man fire dead man snow ser waymar royce come black gare man black come royce say say ser waymar royce ser waymar royce cold like gare gare watch gare gare royce say gare gare fire man fire fire gare say ser waymar fire gare man gare gare fire royce say snow snow\n",
      "le nombre de mots dans le texte apres traitement : 185\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Charger le modèle Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Fonction de preprocessing\n",
    "def preprocess_text(text, frequent_threshold_percent=30, rare_threshold_percent=5):\n",
    "    # Tokenisation du texte\n",
    "    doc = nlp(text.lower())  # Minuscule\n",
    "    \n",
    "    # Extraction des tokens sans stopwords, ponctuations, et sans chiffres\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_digit]\n",
    "\n",
    "    # Appliquer la lemmatisation et filtrer les mots fréquents et rares\n",
    "    lemmatized_tokens = [token.lemma_ for token in tokens]\n",
    "  \n",
    "    #text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    # Compter la fréquence des mots\n",
    "    #word_freq = Counter(lemmatized_tokens)\n",
    "    word_freq = Counter([token for token in lemmatized_tokens])\n",
    "\n",
    "    \n",
    "    # Déterminer la fréquence maximale\n",
    "    max_frequency = max(word_freq.values())\n",
    "    print (f\"max_frequency: {max_frequency}\")\n",
    "    \n",
    "    # Seuils basés sur des pourcentages\n",
    "    frequent_threshold = (frequent_threshold_percent / 100) * max_frequency\n",
    "    rare_threshold = (rare_threshold_percent / 100) * max_frequency\n",
    "\n",
    "    print (f\"la frequence maximale:{frequent_threshold} , rare_threshold: {rare_threshold}\")\n",
    "\n",
    "    # Identification des mots fréquents et rares\n",
    "    frequent_words = {word for word, freq in word_freq.items() if freq > frequent_threshold}\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq <= rare_threshold}\n",
    "\n",
    "    # Appliquer la lemmatisation et filtrer les mots fréquents et rares\n",
    "    lemtokens = [token for token in lemmatized_tokens if token not in frequent_words and token not in rare_words]\n",
    "    \n",
    "    # Retourner le texte nettoyé\n",
    "    return \" \".join(lemtokens)\n",
    "\n",
    "# Lire le fichier txt\n",
    "with open(\"game_of_thrones.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "# Appliquer le prétraitement au texte\n",
    "cleaned_text = preprocess_text(text)\n",
    "word_count = len(cleaned_text.split())\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Texte après prétraitement :\\n\", cleaned_text)  # Affichage des 1000 premiers caractères pour vérifier\n",
    "print (f\"le nombre de mots dans le texte apres traitement : {word_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
